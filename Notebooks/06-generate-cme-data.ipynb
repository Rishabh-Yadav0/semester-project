{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c856c3f8",
   "metadata": {},
   "source": [
    "# 06 - CME Data Generation (LSTM & GAN)\n",
    "\n",
    "This notebook provides tools to generate synthetic CME (Coronal Mass Ejection) time-series data using:\n",
    "\n",
    "- An LSTM sequence generator (autoregressive)\n",
    "- A simple RNN-GAN with LSTM-based generator and discriminator\n",
    "\n",
    "It is designed to plug into the existing project structure. The notebook will:\n",
    "\n",
    "1. Try to load CME files from `../data/` using common filenames.\n",
    "2. Preprocess the data into fixed-length sequences.\n",
    "3. Provide PyTorch implementations for LSTM generator and an LSTM-GAN.\n",
    "4. Train on found CME data, or on a synthetic example if no data is available.\n",
    "\n",
    "Outputs: saved models and generated CSVs under `../data/` if writable.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46163ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and environment checks\n",
    "import os, glob, sys, math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "print('Python', sys.version)\n",
    "print('Torch', torch.__version__)\n",
    "print('Notebook will look for CME files in ../data/ (relative to this notebook)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23aec444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Utility to find data files and load them when present\n",
    "from glob import glob\n",
    "DATA_DIRS = [os.path.join(os.getcwd(),'../data'), os.path.join(os.getcwd(),'./data'), '/mnt/data']\n",
    "candidate_files = []\n",
    "for d in DATA_DIRS:\n",
    "    if os.path.isdir(d):\n",
    "        candidate_files += sorted(glob(os.path.join(d, 'cme*.txt')))\n",
    "        candidate_files += sorted(glob(os.path.join(d, 'cme*.csv')))\n",
    "        candidate_files += sorted(glob(os.path.join(d, '*.txt')))\n",
    "\n",
    "print('Searched directories:', DATA_DIRS)\n",
    "print('Found candidate files (first 20):')\n",
    "for f in candidate_files[:20]:\n",
    "    print(' -', f)\n",
    "\n",
    "# Helper to attempt loading a file as CSV-like\n",
    "def try_load_file(path):\n",
    "    \"\"\"Try to load tabular data from path using pandas with various delimiters.\"\"\"\n",
    "    for sep in [',','\\t',';','|',' ']:\n",
    "        try:\n",
    "            df = pd.read_csv(path, sep=sep, engine='python')\n",
    "            if df.shape[0] > 1:\n",
    "                return df\n",
    "        except Exception:\n",
    "            pass\n",
    "    # try numpy loadtxt\n",
    "    try:\n",
    "        arr = np.loadtxt(path)\n",
    "        return pd.DataFrame(arr)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# Try loading first candidate to inspect\n",
    "sample_df = None\n",
    "if candidate_files:\n",
    "    sample_df = try_load_file(candidate_files[0])\n",
    "    print('\\nLoaded sample shape:', None if sample_df is None else sample_df.shape)\n",
    "    if sample_df is not None:\n",
    "        display(sample_df.head())\n",
    "else:\n",
    "    print('\\nNo candidate CME data files were found in the searched paths.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7887e3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: If no real data, create synthetic CME-like time series for demonstration\n",
    "def make_synthetic_cme(n_series=200, seq_len=128, n_features=1, random_seed=42):\n",
    "    rng = np.random.RandomState(random_seed)\n",
    "    data = []\n",
    "    for i in range(n_series):\n",
    "        t = np.linspace(0, 20, seq_len)\n",
    "        # base: slow variation + burst(s)\n",
    "        base = 0.5*np.sin(0.2*t + rng.randn()*0.5) + 0.1*rng.randn(seq_len)\n",
    "        # add a burst simulating CME intensity/time-localized event\n",
    "        center = rng.uniform(4,16)\n",
    "        width = rng.uniform(0.2,2.0)\n",
    "        burst = rng.uniform(0.5,3.0) * np.exp(-0.5*((t-center)/width)**2)\n",
    "        seq = base + burst\n",
    "        data.append(seq.reshape(seq_len, n_features))\n",
    "    return np.stack(data)\n",
    "\n",
    "if sample_df is None:\n",
    "    print('Creating synthetic dataset for demo...')\n",
    "    data = make_synthetic_cme(n_series=512, seq_len=128)\n",
    "else:\n",
    "    # convert the sample_df into sequences: flatten columns to numeric and create sliding windows\n",
    "    print('Converting loaded file into sequences.\\nYou may want to adapt this cell to your data format.')\n",
    "    arr = sample_df.select_dtypes(include=[np.number]).values\n",
    "    if arr.ndim == 1:\n",
    "        arr = arr.reshape(-1,1)\n",
    "    # create overlapping windows\n",
    "    seq_len = 128\n",
    "    step = 64\n",
    "    seqs = []\n",
    "    for start in range(0, max(1, arr.shape[0]-seq_len), step):\n",
    "        seqs.append(arr[start:start+seq_len])\n",
    "    if not seqs:\n",
    "        # fallback: reshape into multiple sequences if possible\n",
    "        L = arr.shape[0]\n",
    "        n = max(1, L // seq_len)\n",
    "        arr2 = arr[:n*seq_len].reshape(n, seq_len, arr.shape[1])\n",
    "        seqs = list(arr2)\n",
    "    data = np.stack(seqs)\n",
    "\n",
    "print('Data shape (n_series, seq_len, n_features):', data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe216d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Dataset and preprocessing utilities\n",
    "class CMEDataset(Dataset):\n",
    "    def __init__(self, data_array):\n",
    "        # data_array: (N, T, F)\n",
    "        self.x = torch.tensor(data_array, dtype=torch.float32)\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx]\n",
    "\n",
    "# normalization helpers\n",
    "def fit_scaler(data):\n",
    "    mu = data.mean((0,1), keepdims=True)\n",
    "    std = data.std((0,1), keepdims=True) + 1e-6\n",
    "    return mu, std\n",
    "\n",
    "def apply_scaler(data, mu, std):\n",
    "    return (data - mu) / std\n",
    "\n",
    "mu, std = fit_scaler(data)\n",
    "print('mu/std shapes:', mu.shape, std.shape)\n",
    "data_scaled = apply_scaler(data, mu, std)\n",
    "train_loader = DataLoader(CMEDataset(data_scaled), batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d027ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: LSTM autoregressive generator (predict next-step) and sampling helper\n",
    "class LSTMAutoRegressor(nn.Module):\n",
    "    def __init__(self, n_features=1, hidden_size=64, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(n_features, hidden_size, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, n_features)\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, F)\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc(out)\n",
    "\n",
    "# training loop for teacher-forcing next-step prediction\n",
    "def train_lstm_ar(model, data_loader, n_epochs=20, lr=1e-3, device='cpu'):\n",
    "    model = model.to(device)\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    for epoch in range(n_epochs):\n",
    "        tot_loss = 0.0\n",
    "        model.train()\n",
    "        for batch in data_loader:\n",
    "            batch = batch.to(device)\n",
    "            opt.zero_grad()\n",
    "            out = model(batch)\n",
    "            loss = loss_fn(out, batch)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            tot_loss += loss.item() * batch.size(0)\n",
    "        print(f'Epoch {epoch+1}/{n_epochs}  loss={tot_loss/len(data_loader.dataset):.6f}')\n",
    "    return model\n",
    "\n",
    "# sampling function: autoregressively generate a sequence from seed\n",
    "def sample_lstm_ar(model, seed, length=128, device='cpu'):\n",
    "    model = model.to(device).eval()\n",
    "    seed = torch.tensor(seed, dtype=torch.float32).unsqueeze(0).to(device)  # (1,T0,F)\n",
    "    generated = seed.clone()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(length - seed.shape[1]):\n",
    "            out = model(generated)  # (1,curT,F)\n",
    "            next_step = out[:, -1:, :]\n",
    "            generated = torch.cat([generated, next_step], dim=1)\n",
    "    return generated.squeeze(0).cpu().numpy()\n",
    "\n",
    "# create and (optionally) train LSTM AR\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using device', device)\n",
    "lstm_model = LSTMAutoRegressor(n_features=data.shape[2], hidden_size=128, n_layers=2)\n",
    "# train quickly for demo (you can increase epochs)\n",
    "lstm_model = train_lstm_ar(lstm_model, train_loader, n_epochs=8, lr=1e-3, device=device)\n",
    "\n",
    "# save model\n",
    "os.makedirs('../data', exist_ok=True)\n",
    "torch.save({'model_state': lstm_model.state_dict(), 'mu': mu, 'std': std}, '../data/lstm_cme_gen.pt')\n",
    "print('Saved LSTM model to ../data/lstm_cme_gen.pt')\n",
    "\n",
    "# sample and inverse scale for quick preview\n",
    "seed = data_scaled[0][:32]\n",
    "gen = sample_lstm_ar(lstm_model, seed, length=128, device=device)\n",
    "gen_inv = gen * std + mu\n",
    "print('Generated sequence shape:', gen_inv.shape)\n",
    "plt.plot(gen_inv[:,0])\n",
    "plt.title('LSTM generated (inverse-scaled) - feature 0')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde01c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Simple LSTM-GAN (generator produces sequences, discriminator judges sequences)\n",
    "class LSTMGenerator(nn.Module):\n",
    "    def __init__(self, z_dim=32, seq_len=128, n_features=1, hidden=128, n_layers=1):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.fc = nn.Linear(z_dim, seq_len * n_features)\n",
    "        # optionally could use LSTM decoder, but for simplicity we map noise to sequence\n",
    "        # a small LSTM for refinement\n",
    "        self.lstm = nn.LSTM(n_features, hidden, n_layers, batch_first=True)\n",
    "        self.head = nn.Linear(hidden, n_features)\n",
    "    def forward(self, z):\n",
    "        # z: (B, z_dim)\n",
    "        B = z.size(0)\n",
    "        x = self.fc(z).view(B, self.seq_len, -1)\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.head(out)\n",
    "\n",
    "class LSTMDiscriminator(nn.Module):\n",
    "    def __init__(self, n_features=1, hidden=128, n_layers=1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(n_features, hidden, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden, 1)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        # use last hidden output\n",
    "        last = out[:, -1, :]\n",
    "        return self.fc(last)\n",
    "\n",
    "# Loss and training loop\n",
    "z_dim = 64\n",
    "G = LSTMGenerator(z_dim=z_dim, seq_len=data.shape[1], n_features=data.shape[2], hidden=128).to(device)\n",
    "D = LSTMDiscriminator(n_features=data.shape[2], hidden=128).to(device)\n",
    "optG = optim.Adam(G.parameters(), lr=2e-4)\n",
    "optD = optim.Adam(D.parameters(), lr=2e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "real_label = 1.0\n",
    "fake_label = 0.0\n",
    "\n",
    "def train_gan(G, D, loader, n_epochs=30, z_dim=64, device='cpu'):\n",
    "    for epoch in range(n_epochs):\n",
    "        G.train(); D.train()\n",
    "        tot_d_loss = 0.0; tot_g_loss = 0.0\n",
    "        for real in loader:\n",
    "            real = real.to(device)\n",
    "            B = real.size(0)\n",
    "            # train discriminator\n",
    "            optD.zero_grad()\n",
    "            z = torch.randn(B, z_dim, device=device)\n",
    "            fake = G(z)\n",
    "            # real\n",
    "            out_real = D(real).view(-1)\n",
    "            out_fake = D(fake.detach()).view(-1)\n",
    "            lossD = criterion(out_real, torch.ones_like(out_real)) + criterion(out_fake, torch.zeros_like(out_fake))\n",
    "            lossD.backward()\n",
    "            optD.step()\n",
    "            # train generator\n",
    "            optG.zero_grad()\n",
    "            z2 = torch.randn(B, z_dim, device=device)\n",
    "            fake2 = G(z2)\n",
    "            out_fake2 = D(fake2).view(-1)\n",
    "            lossG = criterion(out_fake2, torch.ones_like(out_fake2))\n",
    "            lossG.backward()\n",
    "            optG.step()\n",
    "            tot_d_loss += lossD.item()*B\n",
    "            tot_g_loss += lossG.item()*B\n",
    "        print(f'Epoch {epoch+1}/{n_epochs}  D_loss={tot_d_loss/len(loader.dataset):.6f}  G_loss={tot_g_loss/len(loader.dataset):.6f}')\n",
    "    return G, D\n",
    "\n",
    "# Train GAN (short run for demo)\n",
    "G, D = train_gan(G, D, train_loader, n_epochs=25, z_dim=z_dim, device=device)\n",
    "# Save GAN\n",
    "torch.save({'G_state': G.state_dict(), 'D_state': D.state_dict(), 'mu': mu, 'std': std}, '../data/gan_cme_gen.pt')\n",
    "print('Saved GAN models to ../data/gan_cme_gen.pt')\n",
    "\n",
    "# Generate samples from GAN and plot\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(4, z_dim, device=device)\n",
    "    fake = G(z).cpu().numpy()\n",
    "    fake_inv = fake * std + mu\n",
    "    for i in range(fake_inv.shape[0]):\n",
    "        plt.plot(fake_inv[i,:,0], alpha=0.9)\n",
    "    plt.title('GAN-generated sequences (inverse-scaled)')\n",
    "    plt.show()\n",
    "\n",
    "# Save generated samples to CSV\n",
    "gen_samples = fake_inv.reshape(fake_inv.shape[0], -1)\n",
    "pd.DataFrame(gen_samples).to_csv('../data/gan_generated_cmes.csv', index=False)\n",
    "print('Saved sample GAN-generated CSV to ../data/gan_generated_cmes.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500e7f1a",
   "metadata": {},
   "source": [
    "## Notes & Next steps\n",
    "\n",
    "- The architectures and training loops are intentionally simple and meant as a starting point. For production-quality synthetic time-series generation consider:\n",
    "  - TimeGAN or GANs with seq2seq LSTM decoders.\n",
    "  - Conditional generation (condition on parameters or classes).\n",
    "  - Adding gradient-penalty or WGAN-GP for stability.\n",
    "  - More careful hyperparameter search, early stopping, and logging.\n",
    "\n",
    "- To integrate with your project, replace the data loading cell with the exact reader for your CME files (the earlier notebooks reference `../data/cme*.txt`).\n",
    "- Generated CSVs and model checkpoints are written to `../data/` so downstream notebooks can pick them up.\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
